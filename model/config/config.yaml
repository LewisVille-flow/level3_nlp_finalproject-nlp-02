# config 템플릿 (수정 X)
name: test
seed: 42
train:
  model_name: 'beomi/KoAlpaca-Polyglot-12.8B'
  epoch: 1
  batch_size: 8
  LR: 
    name: WarmupDecayLR # LambdaLR, StepLR, CyclicLR, ExponentialLR, WarmupConstantLR , WarmupDecayLR
    lr: 0.0001
    base: 20 # CyclicLR를 쓴다면, LR에 대한 min_lr을 적어주세요(20 == LR/20)
    max: 5 # CyclicLR를 쓴다면, LR에 대한 max_lr을 적어주세요(5 == LR/5)
    step_up: 5 # CyclicLR를 쓴다면, warmup steps를 적어주세요
    step_down: 5 # CyclicLR를 쓴다면, cooldown steps를 적어주세요 (단, up+down은 epoch과 동일해야 함)
    warmupconstantLR_step: 3
    warmupdecayLR_warmup:  1 # step 기준으로 계산
    warmupdecayLR_total:  100000 # step 기준으로 계산
    interval: step # epoch
  prompts: # topic_title_summarize, topic_title_context
  token_max_len: 512
  patience: 5
  test_size: 0.7
  save_top_k: 3         # Inference할 top k개의 모델.
  halfprecision: True
  gradient_accumulation: 1    # 배치사이즈=16 이고 이 옵션이 2일때, batch=32와 같은 효과.
  korean_first: True
select_DC:
  - # data_cleaning_demo
select_DA:
  - # data_augmentation_demo
adapt:
  peft: QLoRA #,LoRAFull, LoRAfp16, original
  r: 8
  alpha: 32
  dropout: 0.05
inference:
  model_path: # '/opt/ml/level3_nlp_finalproject-nlp-02/model/results/07-17_22:01:03_test_bs2_max1600_test/checkpoints/epoch: 0 - loss: 0.2115'
wandb:
  id: navistwon97
option:
  early_stop: False